{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# lib\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "# from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "import math\n",
    "from lr_utils import load_dataset\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    '''\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    Compute the relu of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    '''\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def load_dataset():\n",
    "    np.random.seed(1)\n",
    "    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n",
    "    np.random.seed(2)\n",
    "    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    test_X = test_X.T\n",
    "    test_Y = test_Y.reshape((1, test_Y.shape[0]))\n",
    "    return (train_X, train_Y, test_X, test_Y)\n",
    "\n",
    "def load_cat_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', 'r')\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'][:]) #  train set features\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'][:]) #  train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', 'r')\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'][:]) #  test set features\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'][:]) #  test set labels\n",
    "\n",
    "    classes = np.array(test_dataset['list_classes'][:]) # the list of classes\n",
    "    \n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "    \n",
    "    train_set_x = train_set_x_orig/255\n",
    "    test_set_x = test_set_x_orig/255\n",
    "\n",
    "    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n",
    "\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    '''\n",
    "    This function is used to predict the results of a  n-layer neural network.    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "    print('Accuracy: '  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "\n",
    "   \n",
    "def predict_dec(parameters, X):\n",
    "    '''\n",
    "    Used for plotting decision boundary.    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)    \n",
    "    Returns:\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    '''        \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    '''\n",
    "    Implements the forward propagation (and computes the loss)    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true 'label' vector (containing 0 if cat, 1 if non-cat)\n",
    "    parameters -- python dictionary containing  parameters 'W1', 'b1', 'W2', 'b2', 'W3', 'b3':\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    '''        \n",
    "    # retrieve parameters\n",
    "    W1 = parameters['W1'];\t\t\tb1 = parameters['b1']\n",
    "    W2 = parameters['W2'];\t\t\tb2 = parameters['b2']\n",
    "    W3 = parameters['W3'];\t\t\tb3 = parameters['b3']\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = ( W1 @ X  ) + b1\t;\t\t\ta1 = relu(z1)\n",
    "    z2 = ( W2 @ a1 ) + b2\t;\t\t\ta2 = relu(z2)\n",
    "    z3 = ( W3 @ a2 ) + b3\t;\t\t\ta3 = sigmoid(z3)\n",
    "    \n",
    "    cache = (\tz1, a1, W1, b1,\n",
    "\t\t\t\tz2, a2, W2, b2,\n",
    "\t\t\t\tz3, a3, W3, b3\t)\n",
    "    \n",
    "    return (a3, cache)\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    '''\n",
    "    Implement the backward propagation\n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true 'label' vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = 1./m * (a3 - Y)\n",
    "    dW3 = dz3 @ a2.T\n",
    "    db3 = np.sum(dz3, axis= 1, keepdims= True)\n",
    "    \n",
    "    da2 = W3.T @ dz3\n",
    "    dz2 = da2 * np.int64(a2 > 0)\n",
    "    dW2 = dz2 @ a1.T\n",
    "    db2 = np.sum(dz2, axis= 1, keepdims= True)\n",
    "    \n",
    "    da1 = W2.T @ dz2\n",
    "    dz1 = da1 * np.int64(a1 > 0)\n",
    "    dW1 = dz1 @ X.T\n",
    "    db1 = np.sum(dz1, axis= 1, keepdims= True)\n",
    "    \n",
    "    gradients = {             'dz3': dz3, 'dW3': dW3, 'db3': db3,\n",
    "                  'da2': da2, 'dz2': dz2, 'dW2': dW2, 'db2': db2,\n",
    "                  'da1': da1, 'dz1': dz1, 'dW1': dW1, 'db1': db1  }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Update parameters using gradient descent    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    grads -- python dictionary containing gradients, output of n_model_backward    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "                  parameters['W' + str(i)] = ... \n",
    "                  parameters['b' + str(i)] = ...\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for k in range(L):\n",
    "        parameters['W' + str(k+1)] = parameters['W' + str(k+1)] - learning_rate * grads['dW' + str(k+1)]\n",
    "        parameters['b' + str(k+1)] = parameters['b' + str(k+1)] - learning_rate * grads['db' + str(k+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def compute_loss(a3, Y):    \n",
    "    '''\n",
    "    Implement the loss function\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- 'true' labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    loss - value of the loss function\n",
    "    '''    \n",
    "    m = Y.shape[1]\n",
    "    logprobs = ( -np.log(a3) * Y )  +  ( -np.log(1 - a3) * (1 - Y) )\n",
    "    loss = 1./m * np.nansum(logprobs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matlplot window setting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrating modules to structure the Neural Network\n",
    "\n",
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
    "    '''\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
    "    learning_rate -- learning rate for gradient descent \n",
    "    num_iterations -- number of iterations to run gradient descent\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    '''\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]  # hard coded for our specific purpose\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the loss\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------------------- Unit Testing: parameters value ------------------------------------\n",
      "\n",
      "W1 = \n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "b1 = \n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "W2 = [[0. 0.]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# zero initialization parameters\n",
    "\n",
    "def initialize_parameters_zeros(layer_dim):\n",
    "    '''\n",
    "    Initializing the parameters with zero values prior undergoing learning\n",
    "    Arguments:\n",
    "    layer_dim -- python array (list) containing the size of each layer.    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    '''    \n",
    "    parameters = {}\n",
    "    L = len(layer_dim)   \n",
    "    \n",
    "    for l in range(1,L):        \n",
    "        parameters[ 'W'+str(l) ] = np.zeros( (layer_dim[l],layer_dim[l-1]) )\n",
    "        parameters[ 'b'+str(l) ] = np.zeros( (layer_dim[l],1) )\n",
    "    \n",
    "    return(parameters)\n",
    "\n",
    "# function testing\n",
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print('\\n\\n---------------------------------- Unit Testing: parameters value ------------------------------------\\n')\n",
    "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
    "print(\"\\nb1 = \\n\" + str(parameters[\"b1\"]))\n",
    "print(\"\\nW2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
